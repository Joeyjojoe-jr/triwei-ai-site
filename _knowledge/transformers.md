---
layout: page
title: "Transformers"
description: Attention-based architecture behind most modern language models.
---
Transformers process tokens in parallel and use self-attention to model relationships across sequence positions.

## Core Components

- Token embeddings
- Multi-head self-attention
- Feed-forward layers
- Residual connections and layer normalization

## Why It Matters

Transformer scaling unlocked large language models, multimodal systems, and modern AI assistants.
