---
layout: page
title: "Transformers"
permalink: /knowledge/transformers/
description: Attention-based architecture behind most modern language models.
---
<!-- Add sub-sections over time as your understanding deepens. -->
Transformers process tokens in parallel and use self-attention to model relationships across sequence positions.

## Core Components

- Token embeddings
- Multi-head self-attention
- Feed-forward layers
- Residual connections and layer normalization

## Why It Matters

Transformer scaling unlocked large language models, multimodal systems, and many modern AI assistants.
